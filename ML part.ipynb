{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import norm, kurtosis\n",
    "from scipy.stats import skew\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the real data for the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_returns_saved_real = pd.read_csv('Real_Classifier.xlsx') \n",
    "n2 = len(data_returns_saved_real)\n",
    "list_num = list(data_returns_saved_real.columns)\n",
    "N2 = len(list_num)\n",
    "data_returns = data_returns_saved_real\n",
    "data_returns_saved_real2 = data_returns_saved_real[data_returns_saved_real.columns[1:]]\n",
    "data_returns_saved_real2 = data_returns_saved_real2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num = list(data_returns_saved_real2.columns)\n",
    "N2 = len(list_num)\n",
    "data_volatility_saved_real = data_returns_saved_real2\n",
    "l2 = len(data_returns_saved_real.columns)\n",
    "s2 = len(data_returns_saved_real2)\n",
    "L2 = [[0]+[100]*(l2-1)]\n",
    "df2 = pd.DataFrame(L2, columns = data_returns_saved_real2.columns )\n",
    "data_volatility_saved_real=df2.append(data_volatility_saved_real,ignore_index=True)\n",
    "\n",
    "for num in list_num:\n",
    "    for i in range(1,s2+1):\n",
    "        data_volatility_saved_real[num].iloc[i] = data_volatility_saved_real[num].iloc[i-1]*(1+data_volatility_saved_real[num].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_volatility_saved_real = data_volatility_saved_real[data_volatility_saved_real.columns[1:]]\n",
    "data_clf_real = pd.DataFrame({\"Name\" : list_num, \"Mean\" : N2*[0] , \"Stdev(chg)\": N2*[0], \"Skewness\": N2*[0], \"Kurtosis\": N2*[0], \"Autocorrelation (Stdev)\": N2*[0], \"Autocorrelation (chgs)\": N2*[0], \"Simulated\":  N2*[0]})\n",
    "list_num = list(data_volatility_saved_real.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for num in list_num:\n",
    "    data_clf_real[\"Mean\"].iloc[j] = data_volatility_saved_real[num].mean() \n",
    "    data_clf_real[\"Stdev(chg)\"].iloc[j] = data_returns_saved_real2[num].std()\n",
    "    data_clf_real[\"Skewness\"].iloc[j] = skew(data_returns_saved_real[num])\n",
    "    data_clf_real[\"Kurtosis\"].iloc[j] = kurtosis(data_returns_saved_real2[num])\n",
    "    data_clf_real[\"Autocorrelation (Stdev)\"].iloc[j] = np.corrcoef(data_volatility_saved_real[num].iloc[range(0,s2)],data_volatility_saved_real[num].iloc[range(1,s2+1)])[0][1]\n",
    "    data_clf_real[\"Autocorrelation (chgs)\"].iloc[j] = np.corrcoef(np.array(list(data_returns_saved_real2[num])[:s2-1]),np.array(list(data_returns_saved_real2[num])[1:s2]))[0][1]\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clf_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clf_real.to_csv('data_clf_real.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present the process for one path of returns simulated from the rough fSV model and for T = 100 and N = 600, but we proceed exactly the same for other models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_returns_saved_simulated_rough = pd.read_csv('data_clf_simulated_fSV_100days_several_paths.xlsx') \n",
    "n2_rough = len(data_returns_saved_simulated_rough)\n",
    "list_num_rough = list(data_returns_saved_simulated_rough.columns)\n",
    "N2_rough = len(list_num_rough)\n",
    "data_returns_rough_fSV = data_returns_saved_simulated_rough\n",
    "data_returns_saved_simulated_rough2 = data_returns_saved_simulated_rough[data_returns_saved_simulated_rough.columns[1:]]\n",
    "data_returns_saved_simulated_rough2 = data_returns_saved_simulated_rough2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_rough = list(data_returns_saved_simulated_rough2.columns)\n",
    "N2_rough = len(list_num_rough)\n",
    "data_volatility_saved_simulated_rough = data_returns_saved_simulated_rough2\n",
    "l2_rough = len(data_returns_saved_simulated_rough2.columns)\n",
    "s2_rough = len(data_returns_saved_simulated_rough2)\n",
    "L2_rough = [[0]+[100]*(l2_rough-1)]\n",
    "df2_rough = pd.DataFrame(L2_rough, columns = data_returns_saved_simulated_rough2.columns )\n",
    "data_volatility_saved_simulated_rough=df2_rough.append(data_volatility_saved_simulated_rough,ignore_index=True)\n",
    "\n",
    "for num in list_num_rough:\n",
    "    for i in range(1,s2_rough+1):\n",
    "        data_volatility_saved_simulated_rough[num].iloc[i] = data_volatility_saved_simulated_rough[num].iloc[i-1]*(1+data_volatility_saved_simulated_rough[num].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_volatility_saved_simulated_rough = data_volatility_saved_simulated_rough[data_volatility_saved_simulated_rough.columns[1:]]\n",
    "data_clf_simulated_rough = pd.DataFrame({\"Name\" : list_num_rough, \"Mean\" : N2_rough*[0] , \"Stdev(chg)\": N2_rough*[0], \"Skewness\": N2_rough*[0], \"Kurtosis\": N2_rough*[0], \"Autocorrelation (Stdev)\": N2_rough*[0], \"Autocorrelation (chgs)\": N2_rough*[0], \"Simulated\":  N2_rough*[1]})\n",
    "list_num_rough = list(data_volatility_saved_simulated_rough.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for num in list_num_rough:\n",
    "    data_clf_simulated_rough[\"Mean\"].iloc[j] = data_volatility_saved_simulated_rough[num].mean() \n",
    "    data_clf_simulated_rough[\"Stdev(chg)\"].iloc[j] = data_returns_saved_simulated_rough2[num].std()\n",
    "    data_clf_simulated_rough[\"Skewness\"].iloc[j] = skew(data_returns_saved_simulated_rough2[num])\n",
    "    data_clf_simulated_rough[\"Kurtosis\"].iloc[j] = kurtosis(data_returns_saved_simulated_rough2[num])\n",
    "    data_clf_simulated_rough[\"Autocorrelation (Stdev)\"].iloc[j] = np.corrcoef(data_volatility_saved_simulated_rough[num].iloc[range(0,s2_rough)],data_volatility_saved_simulated_rough[num].iloc[range(1,s2_rough+1)])[0][1]\n",
    "    data_clf_simulated_rough[\"Autocorrelation (chgs)\"].iloc[j] = np.corrcoef(np.array(list(data_returns_saved_simulated_rough2[num])[:s2_rough-1]),np.array(list(data_returns_saved_simulated_rough2[num])[1:s2_rough]))[0][1]\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clf_simulated_rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clf_simulated_rough.to_csv('data_clf_simulated_fSV_20days_several_paths.xlsx') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stored the data to keep the values and not to launch each time the code before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clf_simulated_rough = pd.read_csv('data_clf_simulated_fSV_20days_several_paths.xlsx'')\n",
    "data_clf_real = pd.read_csv('data_clf_real.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_rough = [data_clf_real, data_clf_simulated_rough]\n",
    "result_rough = pd.concat(frames_rough,ignore_index=True)\n",
    "df_rough = result_rough[result_rough.columns[1:]]\n",
    "df_rough = df_rough[df_rough.columns][:(len(df_rough)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_with_nan_rough = df_rough.index[df_rough.isnull().any(axis=1)]\n",
    "df_rough.drop(index_with_nan_rough,0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data,N):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    train_indices = shuffled_indices[:N]\n",
    "    test_indices = shuffled_indices[N:]\n",
    "    return(data.iloc[train_indices],data.iloc[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_rough = int(len(df_rough)*0.7)\n",
    "train_dataset_rough,test_dataset_rough = split_data(df_rough,N_rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_rough.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_rough.columns[1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 is real data\n",
    "#1 is simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_train_rough =  np.array(train_dataset_rough[train_dataset_rough.columns[1:7]]).reshape(-1, 6)\n",
    "z_train_rough =  np.array(train_dataset_rough[train_dataset_rough.columns[7]]).reshape(-1, 1)\n",
    "V_test_rough = np.array(test_dataset_rough[test_dataset_rough.columns[1:7]]).reshape(-1, 6)\n",
    "z_test_rough = np.array(test_dataset_rough[test_dataset_rough.columns[7]]).reshape(-1, 1)\n",
    "scaler_rough = StandardScaler()\n",
    "V_train_scaled_rough = scaler_rough.fit_transform(V_train_rough.reshape(-1, 6))\n",
    "V_test_scaled_rough = scaler_rough.transform(V_test_rough.reshape(-1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP tuning\n",
    "parameters = {'solver': ['adam'],'activation': ['logistic'], 'max_iter': [1000,1400], 'alpha': 10.0 ** -np.arange(0, 5), 'hidden_layer_sizes':np.arange(50, 250,50)}\n",
    "grid_MLP = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1,cv = 3)\n",
    "clf_MLPs_rough = grid_MLP.fit(V_train_scaled_rough, z_train_rough)\n",
    "clf_MLP_rough = clf_MLPs_rough.best_estimator_\n",
    "z_train_predict_MLP_rough = clf_MLP_rough.predict(V_train_scaled_rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_MLP_rough.score(V_train_scaled_rough,z_train_rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_MLP_rough.score(V_test_scaled_rough,z_test_rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF_rough = RandomForestClassifier()\n",
    "params_RF_rough = {'n_estimators': [100, 200, 300],'max_depth': [20,50,100], 'random_state': [0,1,2], 'max_features':['sqrt', 'log2',4]}\n",
    "grid_RF_rough = GridSearchCV(model_RF_rough, params_RF_rough, n_jobs=-1,cv = 3)\n",
    "grid_RF_rough.fit(V_train_rough, z_train_rough)\n",
    "clf_RF_rough = grid_RF_rough.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF_rough.score(V_train_rough,z_train_rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF_rough.score(V_test_rough,z_test_rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "parameters = {'axes.labelsize': 20,\n",
    "          'axes.titlesize': 20,\n",
    "          'legend.fontsize': 30,\n",
    "          'xtick.labelsize': 20,\n",
    "          'ytick.labelsize': 20}\n",
    "plt.rcParams.update(parameters) \n",
    "plot_confusion_matrix(clf_MLP_rough,V_test_scaled_rough, z_test_rough, cmap=plt.cm.Blues) \n",
    "plot_confusion_matrix(clf_RF_rough,V_test_rough, z_test_rough, cmap=plt.cm.Blues) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plot_tree(clf_RF_rough.estimators_[0], \n",
    "          feature_names=list(df.columns[2:8]),\n",
    "          class_names=list(df.columns[8]), \n",
    "          filled=True, impurity=True, \n",
    "          rounded=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
